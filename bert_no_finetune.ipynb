{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset as lds\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from dataset.loader import ContractNLIExample\n",
    "import json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tanve\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\tanve\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "c:\\Users\\tanve\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load dataset\n",
    "dataset = ContractNLIExample.load(json.load(open('../dataset/contract-nli/dev.json','r')))\n",
    "# Use BERT base model without fine-tuning\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"snli\")\n",
    "# Filter out examples with label -1\n",
    "ds = ds.filter(lambda example: example['label'] != -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "646937e2b0c342c383149dda7362ede4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9824 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e933ecf9b3f4ee78e0bab690f3ec7f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9842 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddaa12fe85c449139f80e2be112bf4cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/549367 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    # Tokenize the inputs\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"premise\"],\n",
    "        examples[\"hypothesis\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "    \n",
    "    # Use the original labels directly\n",
    "    tokenized[\"labels\"] = examples[\"label\"]\n",
    "    return tokenized\n",
    "\n",
    "tokenized_ds = ds.map(tokenize_function, batched=True)\n",
    "tokenized_ds.set_format(\"torch\", columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "test_dataset = tokenized_ds[\"test\"]\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, original_dataset):\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Original label mapping from the dataset\n",
    "    original_label_map = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n",
    "    \n",
    "    progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=\"Evaluating\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in progress_bar:\n",
    "            inputs = {\n",
    "                'input_ids': batch['input_ids'].to(device),\n",
    "                'attention_mask': batch['attention_mask'].to(device),\n",
    "            }\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            batch_preds = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            all_predictions.extend(batch_preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # Calculate live metrics\n",
    "            current_accuracy = accuracy_score(all_labels, all_predictions)\n",
    "            current_f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "            \n",
    "            progress_bar.set_postfix({'Accuracy': f'{current_accuracy:.4f}', 'F1': f'{current_f1:.4f}'})\n",
    "            \n",
    "            # Print examples for the first batch\n",
    "            if batch_idx == 0:\n",
    "                for j in range(min(10, len(batch['input_ids']))):\n",
    "                    idx = batch_idx * dataloader.batch_size + j\n",
    "                    if idx < len(original_dataset):\n",
    "                        premise = original_dataset[idx]['premise']\n",
    "                        hypothesis = original_dataset[idx]['hypothesis']\n",
    "                        true_label_idx = original_dataset[idx]['label']\n",
    "                        true_label = original_label_map[true_label_idx]\n",
    "                        pred_label = original_label_map[batch_preds[j].item()]\n",
    "                        \n",
    "                        print(f\"\\nExample {idx + 1}:\")\n",
    "                        print(f\"Premise: {premise}\")\n",
    "                        print(f\"Hypothesis: {hypothesis}\")\n",
    "                        print(f\"True label: {true_label}\")\n",
    "                        print(f\"Predicted label: {pred_label}\")\n",
    "                        print(f\"Logits: {logits[j].cpu().tolist()}\")\n",
    "                        print(\"-\" * 50)\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    f1_macro = f1_score(all_labels, all_predictions, average='macro')\n",
    "    f1_weighted = f1_score(all_labels, all_predictions, average='weighted')\n",
    "    \n",
    "    print(f\"\\nFinal Results:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score (Macro): {f1_macro:.4f}\")\n",
    "    print(f\"F1 Score (Weighted): {f1_weighted:.4f}\")\n",
    "    \n",
    "    return all_predictions, all_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 1/614 [00:01<15:53,  1.56s/it, Accuracy=0.3125, F1=0.1786]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "Premise: This church choir sings to the masses as they sing joyous songs from the book at a church.\n",
      "Hypothesis: The church has cracks in the ceiling.\n",
      "True label: neutral\n",
      "Predicted label: contradiction\n",
      "Logits: [-0.15072835981845856, -0.055166445672512054, 0.017433125525712967]\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 2:\n",
      "Premise: This church choir sings to the masses as they sing joyous songs from the book at a church.\n",
      "Hypothesis: The church is filled with song.\n",
      "True label: entailment\n",
      "Predicted label: contradiction\n",
      "Logits: [-0.16454318165779114, -0.07607656717300415, 0.033789150416851044]\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 3:\n",
      "Premise: This church choir sings to the masses as they sing joyous songs from the book at a church.\n",
      "Hypothesis: A choir singing at a baseball game.\n",
      "True label: contradiction\n",
      "Predicted label: contradiction\n",
      "Logits: [-0.086623415350914, -0.09536000341176987, 0.1066807210445404]\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 4:\n",
      "Premise: A woman with a green headscarf, blue shirt and a very big grin.\n",
      "Hypothesis: The woman is young.\n",
      "True label: neutral\n",
      "Predicted label: contradiction\n",
      "Logits: [-0.1859463006258011, -0.16026592254638672, 0.06586621701717377]\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 5:\n",
      "Premise: A woman with a green headscarf, blue shirt and a very big grin.\n",
      "Hypothesis: The woman is very happy.\n",
      "True label: entailment\n",
      "Predicted label: contradiction\n",
      "Logits: [-0.20810258388519287, -0.15203577280044556, 0.04761400446295738]\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 6:\n",
      "Premise: A woman with a green headscarf, blue shirt and a very big grin.\n",
      "Hypothesis: The woman has been shot.\n",
      "True label: contradiction\n",
      "Predicted label: contradiction\n",
      "Logits: [-0.19471099972724915, -0.10254751890897751, 0.08872558176517487]\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 7:\n",
      "Premise: An old man with a package poses in front of an advertisement.\n",
      "Hypothesis: A man poses in front of an ad.\n",
      "True label: entailment\n",
      "Predicted label: contradiction\n",
      "Logits: [-0.03449299931526184, -0.14833033084869385, 0.13297323882579803]\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 8:\n",
      "Premise: An old man with a package poses in front of an advertisement.\n",
      "Hypothesis: A man poses in front of an ad for beer.\n",
      "True label: neutral\n",
      "Predicted label: contradiction\n",
      "Logits: [-0.02676451951265335, -0.1772572100162506, 0.12534120678901672]\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 9:\n",
      "Premise: An old man with a package poses in front of an advertisement.\n",
      "Hypothesis: A man walks by an ad.\n",
      "True label: contradiction\n",
      "Predicted label: contradiction\n",
      "Logits: [-0.10403114557266235, -0.13152897357940674, 0.09164196252822876]\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 10:\n",
      "Premise: A statue at a museum that no seems to be looking at.\n",
      "Hypothesis: The statue is offensive and people are mad that it is on display.\n",
      "True label: neutral\n",
      "Predicted label: contradiction\n",
      "Logits: [-0.21833300590515137, -0.11001264303922653, 0.023060904815793037]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  21%|██▏       | 131/614 [00:52<03:14,  2.48it/s, Accuracy=0.3230, F1=0.1640]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Run evaluation\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m predictions, true_labels \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 27\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, dataloader, original_dataset)\u001b[0m\n\u001b[0;32m     23\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m     25\u001b[0m batch_preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m all_predictions\u001b[38;5;241m.\u001b[39mextend(\u001b[43mbatch_preds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     28\u001b[0m all_labels\u001b[38;5;241m.\u001b[39mextend(labels\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Calculate live metrics\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run evaluation\n",
    "predictions, true_labels = evaluate(model, test_dataloader, ds[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
