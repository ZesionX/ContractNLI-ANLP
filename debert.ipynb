{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset as lds\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from dataset.loader import ContractNLIExample\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tanve\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\tanve\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "c:\\Users\\tanve\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load dataset and model\n",
    "dataset = ContractNLIExample.load(json.load(open('../dataset/contract-nli/dev.json','r')))\n",
    "model_name = \"microsoft/deberta-large-mnli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = lds(\"snli\")\n",
    "# Filter out examples with label -1\n",
    "ds = ds.filter(lambda example: example['label'] != -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f11231178cc24ca4aef5e18f675ef699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/549367 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    # Tokenize the inputs\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"premise\"],\n",
    "        examples[\"hypothesis\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "    \n",
    "    # Remap labels to match the model's expected label mapping\n",
    "    label_remap = {0: 2, 1: 1, 2: 0}  # From SNLI labels to DeBERTa labels\n",
    "    tokenized[\"labels\"] = [label_remap[label] for label in examples[\"label\"]]\n",
    "    return tokenized\n",
    "\n",
    "tokenized_ds = ds.map(tokenize_function, batched=True)\n",
    "tokenized_ds.set_format(\"torch\", columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d203bcb3a0014b799969e9085f028805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9824 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "test_dataset = tokenized_ds[\"test\"]\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_logits(logits):\n",
    "    # Convert logits to CPU and extract values\n",
    "    logits_cpu = logits.cpu().tolist()\n",
    "    \n",
    "    # If the first logit (index 0) is highest, it's entailment\n",
    "    if logits_cpu[0] > logits_cpu[1] and logits_cpu[0] > logits_cpu[2]:\n",
    "        return 0  # entailment\n",
    "    # If the second logit is highest, it's neutral\n",
    "    elif logits_cpu[1] > logits_cpu[0] and logits_cpu[1] > logits_cpu[2]:\n",
    "        return 1  # neutral\n",
    "    # Otherwise, it's contradiction\n",
    "    else:\n",
    "        return 2  # contradiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, original_dataset):\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Model's label mapping\n",
    "    model_label_map = {0: \"contradiction\", 1: \"neutral\", 2: \"entailment\"}\n",
    "    # Original label mapping from the dataset\n",
    "    original_label_map = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n",
    "    \n",
    "    progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=\"Evaluating\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in progress_bar:\n",
    "            inputs = {\n",
    "                'input_ids': batch['input_ids'].to(device),\n",
    "                'attention_mask': batch['attention_mask'].to(device),\n",
    "            }\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            batch_preds = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            all_predictions.extend(batch_preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # Calculate live metrics\n",
    "            current_accuracy = accuracy_score(all_labels, all_predictions)\n",
    "            current_f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "            \n",
    "            progress_bar.set_postfix({'Accuracy': f'{current_accuracy:.4f}', 'F1': f'{current_f1:.4f}'})\n",
    "            \n",
    "            # Print examples for the first batch\n",
    "            if batch_idx == 0:\n",
    "                for j in range(min(10, len(batch['input_ids']))):\n",
    "                    idx = batch_idx * dataloader.batch_size + j\n",
    "                    if idx < len(original_dataset):\n",
    "                        premise = original_dataset[idx]['premise']\n",
    "                        hypothesis = original_dataset[idx]['hypothesis']\n",
    "                        true_label_idx = original_dataset[idx]['label']\n",
    "                        true_label = original_label_map[true_label_idx]\n",
    "                        pred_label = model_label_map[batch_preds[j].item()]\n",
    "                        \n",
    "                        print(f\"\\nExample {idx + 1}:\")\n",
    "                        print(f\"Premise: {premise}\")\n",
    "                        print(f\"Hypothesis: {hypothesis}\")\n",
    "                        print(f\"True label: {true_label}\")\n",
    "                        print(f\"Predicted label: {pred_label}\")\n",
    "                        print(f\"Logits: {logits[j].cpu().tolist()}\")\n",
    "                        print(\"-\" * 50)\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    f1_macro = f1_score(all_labels, all_predictions, average='macro')\n",
    "    f1_weighted = f1_score(all_labels, all_predictions, average='weighted')\n",
    "    \n",
    "    print(f\"\\nFinal Results:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score (Macro): {f1_macro:.4f}\")\n",
    "    print(f\"F1 Score (Weighted): {f1_weighted:.4f}\")\n",
    "    \n",
    "    return all_predictions, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 1/614 [00:02<26:25,  2.59s/it, Accuracy=0.8750, F1=0.8818]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "Premise: This church choir sings to the masses as they sing joyous songs from the book at a church.\n",
      "Hypothesis: The church has cracks in the ceiling.\n",
      "True label: neutral\n",
      "Predicted label: neutral\n",
      "Logits: [-0.23674659430980682, 3.084219217300415, -2.8140945434570312]\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 2:\n",
      "Premise: This church choir sings to the masses as they sing joyous songs from the book at a church.\n",
      "Hypothesis: The church is filled with song.\n",
      "True label: entailment\n",
      "Predicted label: entailment\n",
      "Logits: [-3.5898513793945312, 1.5226945877075195, 2.2780914306640625]\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 3:\n",
      "Premise: This church choir sings to the masses as they sing joyous songs from the book at a church.\n",
      "Hypothesis: A choir singing at a baseball game.\n",
      "True label: contradiction\n",
      "Predicted label: contradiction\n",
      "Logits: [5.034609317779541, -2.1406774520874023, -2.6638550758361816]\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 4:\n",
      "Premise: A woman with a green headscarf, blue shirt and a very big grin.\n",
      "Hypothesis: The woman is young.\n",
      "True label: neutral\n",
      "Predicted label: neutral\n",
      "Logits: [-2.189945936203003, 4.714697360992432, -1.9353079795837402]\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 5:\n",
      "Premise: A woman with a green headscarf, blue shirt and a very big grin.\n",
      "Hypothesis: The woman is very happy.\n",
      "True label: entailment\n",
      "Predicted label: entailment\n",
      "Logits: [-3.8815629482269287, 1.7498241662979126, 2.589092493057251]\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 6:\n",
      "Premise: A woman with a green headscarf, blue shirt and a very big grin.\n",
      "Hypothesis: The woman has been shot.\n",
      "True label: contradiction\n",
      "Predicted label: neutral\n",
      "Logits: [1.4231936931610107, 2.725447654724121, -4.120624542236328]\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 7:\n",
      "Premise: An old man with a package poses in front of an advertisement.\n",
      "Hypothesis: A man poses in front of an ad.\n",
      "True label: entailment\n",
      "Predicted label: entailment\n",
      "Logits: [-2.8998258113861084, -0.7568320035934448, 4.687451362609863]\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 8:\n",
      "Premise: An old man with a package poses in front of an advertisement.\n",
      "Hypothesis: A man poses in front of an ad for beer.\n",
      "True label: neutral\n",
      "Predicted label: neutral\n",
      "Logits: [-0.46577945351600647, 4.11013650894165, -3.3092994689941406]\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 9:\n",
      "Premise: An old man with a package poses in front of an advertisement.\n",
      "Hypothesis: A man walks by an ad.\n",
      "True label: contradiction\n",
      "Predicted label: contradiction\n",
      "Logits: [3.5583438873291016, -1.6448099613189697, -1.7336965799331665]\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 10:\n",
      "Premise: A statue at a museum that no seems to be looking at.\n",
      "Hypothesis: The statue is offensive and people are mad that it is on display.\n",
      "True label: neutral\n",
      "Predicted label: neutral\n",
      "Logits: [0.01233078446239233, 4.428461074829102, -3.9832382202148438]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   1%|          | 6/614 [00:14<24:11,  2.39s/it, Accuracy=0.8854, F1=0.8849]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Run evaluation\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m predictions, true_labels \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 29\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, dataloader, original_dataset)\u001b[0m\n\u001b[0;32m     25\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m     27\u001b[0m batch_preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 29\u001b[0m all_predictions\u001b[38;5;241m.\u001b[39mextend(\u001b[43mbatch_preds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     30\u001b[0m all_labels\u001b[38;5;241m.\u001b[39mextend(labels\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Calculate live metrics\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run evaluation\n",
    "predictions, true_labels = evaluate(model, test_dataloader, ds[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
